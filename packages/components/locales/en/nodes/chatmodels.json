{
    "chatOpenAI": {
        "label": "ChatOpenAI",
        "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "strictToolCalling": {
                "label": "Strict Tool Calling",
                "description": "Whether the model supports the `strict` argument when passing in tools. If not specified, the `strict` argument will not be passed to OpenAI."
            },
            "stopSequence": {
                "label": "Stop Sequence",
                "description": "List of stop words to use when generating. Use comma to separate multiple stop words."
            },
            "basepath": {
                "label": "BasePath"
            },
            "proxyUrl": {
                "label": "Proxy Url"
            },
            "baseOptions": {
                "label": "BaseOptions"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "imageResolution": {
                "label": "Image Resolution",
                "description": "This parameter controls the resolution in which the model views the image.",
                "options": {
                    "low": "Low",
                    "high": "High",
                    "auto": "Auto"
                }
            },
            "reasoning": {
                "label": "Reasoning",
                "description": "Whether the model supports reasoning. Only applicable for reasoning models."
            },
            "reasoningEffort": {
                "label": "Reasoning Effort",
                "description": "Constrains effort on reasoning for reasoning models",
                "options": {
                    "low": "Low",
                    "medium": "Medium",
                    "high": "High"
                }
            },
            "reasoningSummary": {
                "label": "Reasoning Summary",
                "description": "A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process",
                "options": {
                    "auto": "Auto",
                    "concise": "Concise",
                    "detailed": "Detailed"
                }
            }
        }
    },
    "azureChatOpenAI": {
        "label": "Azure ChatOpenAI",
        "description": "Wrapper around Azure OpenAI large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "streaming": {
                "label": "Streaming"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "basepath": {
                "label": "BasePath"
            },
            "baseOptions": {
                "label": "BaseOptions"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "imageResolution": {
                "label": "Image Resolution",
                "description": "This parameter controls the resolution in which the model views the image.",
                "options": {
                    "low": "Low",
                    "high": "High",
                    "auto": "Auto"
                }
            },
            "reasoning": {
                "label": "Reasoning",
                "description": "Whether the model supports reasoning. Only applicable for reasoning models."
            },
            "reasoningEffort": {
                "label": "Reasoning Effort",
                "description": "Constrains effort on reasoning for reasoning models. Only applicable for o1 and o3 models.",
                "options": {
                    "low": "Low",
                    "medium": "Medium",
                    "high": "High"
                }
            },
            "reasoningSummary": {
                "label": "Reasoning Summary",
                "description": "A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process",
                "options": {
                    "auto": "Auto",
                    "concise": "Concise",
                    "detailed": "Detailed"
                }
            }
        }
    },
    "chatAnthropic": {
        "label": "ChatAnthropic",
        "description": "Wrapper around ChatAnthropic large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokensToSample": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top P"
            },
            "topK": {
                "label": "Top K"
            },
            "extendedThinking": {
                "label": "Extended Thinking",
                "description": "Enable extended thinking for reasoning model such as Claude Sonnet 3.7"
            },
            "budgetTokens": {
                "label": "Budget Tokens",
                "description": "Maximum number of tokens Claude is allowed use for its internal reasoning process"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            }
        }
    },
    "chatGoogleGenerativeAI": {
        "label": "ChatGoogleGenerativeAI",
        "description": "Wrapper around Google Gemini large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential",
            "description": "Google Generative AI credential."
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "customModelName": {
                "label": "Custom Model Name",
                "placeholder": "gemini-1.5-pro-exp-0801",
                "description": "Custom model name to use. If provided, it will override the model selected"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxOutputTokens": {
                "label": "Max Output Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "topK": {
                "label": "Top Next Highest Probability Tokens",
                "description": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive"
            },
            "safetySettings": {
                "label": "Safety Settings",
                "description": "Safety settings for the model. Refer to the <a href=\"https://ai.google.dev/gemini-api/docs/safety-settings\">official guide</a> on how to use Safety Settings",
                "harmCategory": {
                    "label": "Harm Category",
                    "options": {
                        "dangerous": "Dangerous",
                        "harassment": "Harassment",
                        "hateSpeech": "Hate Speech",
                        "sexuallyExplicit": "Sexually Explicit",
                        "civicIntegrity": "Civic Integrity"
                    },
                    "descriptions": {
                        "dangerous": "Promotes, facilitates, or encourages harmful acts.",
                        "harassment": "Negative or harmful comments targeting identity and/or protected attributes.",
                        "hateSpeech": "Content that is rude, disrespectful, or profane.",
                        "sexuallyExplicit": "Contains references to sexual acts or other lewd content.",
                        "civicIntegrity": "Election-related queries."
                    }
                },
                "harmBlockThreshold": {
                    "label": "Harm Block Threshold",
                    "options": {
                        "none": "None",
                        "onlyHigh": "Only High",
                        "mediumAndAbove": "Medium and Above",
                        "lowAndAbove": "Low and Above",
                        "unspecified": "Threshold Unspecified (Default Threshold)"
                    },
                    "descriptions": {
                        "none": "Always show regardless of probability of unsafe content",
                        "onlyHigh": "Block when high probability of unsafe content",
                        "mediumAndAbove": "Block when medium or high probability of unsafe content",
                        "lowAndAbove": "Block when low, medium or high probability of unsafe content",
                        "unspecified": "Threshold is unspecified, block using default threshold"
                    }
                }
            },
            "baseUrl": {
                "label": "Base URL",
                "description": "Base URL for the API. Leave empty to use the default."
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            }
        }
    },
    "chatMistralAI": {
        "label": "ChatMistralAI",
        "description": "Wrapper around Mistral large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature",
                "description": "What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxOutputTokens": {
                "label": "Max Output Tokens",
                "description": "The maximum number of tokens to generate in the completion."
            },
            "topP": {
                "label": "Top Probability",
                "description": "Nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
            },
            "randomSeed": {
                "label": "Random Seed",
                "description": "The seed to use for random sampling. If set, different calls will generate deterministic results."
            },
            "safeMode": {
                "label": "Safe Mode",
                "description": "Whether to inject a safety prompt before all conversations."
            },
            "overrideEndpoint": {
                "label": "Override Endpoint"
            }
        }
    },
    "groqChat": {
        "label": "GroqChat",
        "description": "Wrapper around Groq API with LPU Inference Engine",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name",
                "placeholder": "llama3-70b-8192"
            },
            "temperature": {
                "label": "Temperature"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "streaming": {
                "label": "Streaming"
            }
        }
    },
    "chatOllama": {
        "label": "ChatOllama",
        "description": "Chat completion using open-source LLM on Ollama",
        "category": "Chat Models",
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "baseUrl": {
                "label": "Base URL"
            },
            "modelName": {
                "label": "Model Name",
                "placeholder": "llama2"
            },
            "temperature": {
                "label": "Temperature",
                "description": "The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "streaming": {
                "label": "Streaming"
            },
            "jsonMode": {
                "label": "JSON Mode",
                "description": "Coerces model outputs to only return JSON. Specify in the system prompt to return JSON. Ex: Format all responses as JSON object"
            },
            "keepAlive": {
                "label": "Keep Alive",
                "description": "How long to keep connection alive. A duration string (such as \"10m\" or \"24h\")"
            },
            "topP": {
                "label": "Top P",
                "description": "Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "topK": {
                "label": "Top K",
                "description": "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "mirostat": {
                "label": "Mirostat",
                "description": "Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "mirostatEta": {
                "label": "Mirostat ETA",
                "description": "Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "mirostatTau": {
                "label": "Mirostat TAU",
                "description": "Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "numCtx": {
                "label": "Context Window Size",
                "description": "Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "numGpu": {
                "label": "Number of GPU",
                "description": "The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "numThread": {
                "label": "Number of Thread",
                "description": "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "repeatLastN": {
                "label": "Repeat Last N",
                "description": "Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "repeatPenalty": {
                "label": "Repeat Penalty",
                "description": "Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "stop": {
                "label": "Stop Sequence",
                "placeholder": "AI assistant:",
                "description": "Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "tfsZ": {
                "label": "Tail Free Sampling",
                "description": "Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            }
        }
    },
    "chatHuggingFace": {
        "label": "ChatHuggingFace",
        "description": "Wrapper around HuggingFace large language models",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "model": {
                "label": "Model",
                "placeholder": "gpt2",
                "description": "If using own inference endpoint, leave this blank"
            },
            "endpoint": {
                "label": "Endpoint",
                "placeholder": "https://xyz.eu-west-1.aws.endpoints.huggingface.cloud/gpt2",
                "description": "Using your own inference endpoint"
            },
            "temperature": {
                "label": "Temperature",
                "description": "Temperature parameter may not apply to certain model. Please check available model parameters"
            },
            "maxTokens": {
                "label": "Max Tokens",
                "description": "Max Tokens parameter may not apply to certain model. Please check available model parameters"
            },
            "topP": {
                "label": "Top Probability",
                "description": "Top Probability parameter may not apply to certain model. Please check available model parameters"
            },
            "hfTopK": {
                "label": "Top K",
                "description": "Top K parameter may not apply to certain model. Please check available model parameters"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty",
                "description": "Frequency Penalty parameter may not apply to certain model. Please check available model parameters"
            },
            "stop": {
                "label": "Stop Sequence",
                "placeholder": "AI assistant:",
                "description": "Sets the stop sequences to use. Use comma to seperate different sequences."
            }
        }
    },
    "awsChatBedrock": {
        "label": "AWS ChatBedrock",
        "description": "Wrapper around AWS Bedrock large language models that use the Converse API",
        "category": "Chat Models",
        "credential": {
            "label": "AWS Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "region": {
                "label": "Region"
            },
            "model": {
                "label": "Model Name"
            },
            "customModel": {
                "label": "Custom Model Name",
                "description": "If provided, will override model selected from Model Name option"
            },
            "streaming": {
                "label": "Streaming"
            },
            "temperature": {
                "label": "Temperature",
                "description": "Temperature parameter may not apply to certain model. Please check available model parameters"
            },
            "max_tokens_to_sample": {
                "label": "Max Tokens to Sample",
                "description": "Max Tokens parameter may not apply to certain model. Please check available model parameters"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "latencyOptimized": {
                "label": "Latency Optimized",
                "description": "Enable latency optimized configuration for supported models. Refer to the supported <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html\" target=\"_blank\">latecny optimized models</a> for more details."
            }
        }
    },
    "chatCohere": {
        "label": "ChatCohere",
        "description": "Wrapper around Cohere Chat Endpoints",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            }
        }
    }
}
