{
    "chatOpenAI": {
        "label": "ChatOpenAI",
        "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "strictToolCalling": {
                "label": "Strict Tool Calling",
                "description": "Whether the model supports the `strict` argument when passing in tools. If not specified, the `strict` argument will not be passed to OpenAI."
            },
            "stopSequence": {
                "label": "Stop Sequence",
                "description": "List of stop words to use when generating. Use comma to separate multiple stop words."
            },
            "basepath": {
                "label": "BasePath"
            },
            "proxyUrl": {
                "label": "Proxy Url"
            },
            "baseOptions": {
                "label": "BaseOptions"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "imageResolution": {
                "label": "Image Resolution",
                "description": "This parameter controls the resolution in which the model views the image.",
                "options": {
                    "low": "Low",
                    "high": "High",
                    "auto": "Auto"
                }
            },
            "reasoning": {
                "label": "Reasoning",
                "description": "Whether the model supports reasoning. Only applicable for reasoning models."
            },
            "reasoningEffort": {
                "label": "Reasoning Effort",
                "description": "Constrains effort on reasoning for reasoning models",
                "options": {
                    "low": "Low",
                    "medium": "Medium",
                    "high": "High"
                }
            },
            "reasoningSummary": {
                "label": "Reasoning Summary",
                "description": "A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process",
                "options": {
                    "auto": "Auto",
                    "concise": "Concise",
                    "detailed": "Detailed"
                }
            }
        }
    },
    "azureChatOpenAI": {
        "label": "Azure ChatOpenAI",
        "description": "Wrapper around Azure OpenAI large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "streaming": {
                "label": "Streaming"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "basepath": {
                "label": "BasePath"
            },
            "baseOptions": {
                "label": "BaseOptions"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "imageResolution": {
                "label": "Image Resolution",
                "description": "This parameter controls the resolution in which the model views the image.",
                "options": {
                    "low": "Low",
                    "high": "High",
                    "auto": "Auto"
                }
            },
            "reasoning": {
                "label": "Reasoning",
                "description": "Whether the model supports reasoning. Only applicable for reasoning models."
            },
            "reasoningEffort": {
                "label": "Reasoning Effort",
                "description": "Constrains effort on reasoning for reasoning models. Only applicable for o1 and o3 models.",
                "options": {
                    "low": "Low",
                    "medium": "Medium",
                    "high": "High"
                }
            },
            "reasoningSummary": {
                "label": "Reasoning Summary",
                "description": "A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process",
                "options": {
                    "auto": "Auto",
                    "concise": "Concise",
                    "detailed": "Detailed"
                }
            }
        }
    },
    "chatAnthropic": {
        "label": "ChatAnthropic",
        "description": "Wrapper around ChatAnthropic large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokensToSample": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top P"
            },
            "topK": {
                "label": "Top K"
            },
            "extendedThinking": {
                "label": "Extended Thinking",
                "description": "Enable extended thinking for reasoning model such as Claude Sonnet 3.7"
            },
            "budgetTokens": {
                "label": "Budget Tokens",
                "description": "Maximum number of tokens Claude is allowed use for its internal reasoning process"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            }
        }
    },
    "chatGoogleGenerativeAI": {
        "label": "ChatGoogleGenerativeAI",
        "description": "Wrapper around Google Gemini large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential",
            "description": "Google Generative AI credential."
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "customModelName": {
                "label": "Custom Model Name",
                "placeholder": "gemini-1.5-pro-exp-0801",
                "description": "Custom model name to use. If provided, it will override the model selected"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxOutputTokens": {
                "label": "Max Output Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "topK": {
                "label": "Top Next Highest Probability Tokens",
                "description": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive"
            },
            "safetySettings": {
                "label": "Safety Settings",
                "description": "Safety settings for the model. Refer to the <a href=\"https://ai.google.dev/gemini-api/docs/safety-settings\">official guide</a> on how to use Safety Settings",
                "harmCategory": {
                    "label": "Harm Category",
                    "options": {
                        "dangerous": "Dangerous",
                        "harassment": "Harassment",
                        "hateSpeech": "Hate Speech",
                        "sexuallyExplicit": "Sexually Explicit",
                        "civicIntegrity": "Civic Integrity"
                    },
                    "descriptions": {
                        "dangerous": "Promotes, facilitates, or encourages harmful acts.",
                        "harassment": "Negative or harmful comments targeting identity and/or protected attributes.",
                        "hateSpeech": "Content that is rude, disrespectful, or profane.",
                        "sexuallyExplicit": "Contains references to sexual acts or other lewd content.",
                        "civicIntegrity": "Election-related queries."
                    }
                },
                "harmBlockThreshold": {
                    "label": "Harm Block Threshold",
                    "options": {
                        "none": "None",
                        "onlyHigh": "Only High",
                        "mediumAndAbove": "Medium and Above",
                        "lowAndAbove": "Low and Above",
                        "unspecified": "Threshold Unspecified (Default Threshold)"
                    },
                    "descriptions": {
                        "none": "Always show regardless of probability of unsafe content",
                        "onlyHigh": "Block when high probability of unsafe content",
                        "mediumAndAbove": "Block when medium or high probability of unsafe content",
                        "lowAndAbove": "Block when low, medium or high probability of unsafe content",
                        "unspecified": "Threshold is unspecified, block using default threshold"
                    }
                }
            },
            "baseUrl": {
                "label": "Base URL",
                "description": "Base URL for the API. Leave empty to use the default."
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            }
        }
    },
    "chatMistralAI": {
        "label": "ChatMistralAI",
        "description": "Wrapper around Mistral large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature",
                "description": "What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxOutputTokens": {
                "label": "Max Output Tokens",
                "description": "The maximum number of tokens to generate in the completion."
            },
            "topP": {
                "label": "Top Probability",
                "description": "Nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
            },
            "randomSeed": {
                "label": "Random Seed",
                "description": "The seed to use for random sampling. If set, different calls will generate deterministic results."
            },
            "safeMode": {
                "label": "Safe Mode",
                "description": "Whether to inject a safety prompt before all conversations."
            },
            "overrideEndpoint": {
                "label": "Override Endpoint"
            }
        }
    },
    "groqChat": {
        "label": "GroqChat",
        "description": "Wrapper around Groq API with LPU Inference Engine",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name",
                "placeholder": "llama3-70b-8192"
            },
            "temperature": {
                "label": "Temperature"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "streaming": {
                "label": "Streaming"
            }
        }
    },
    "chatOllama": {
        "label": "ChatOllama",
        "description": "Chat completion using open-source LLM on Ollama",
        "category": "Chat Models",
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "baseUrl": {
                "label": "Base URL"
            },
            "modelName": {
                "label": "Model Name",
                "placeholder": "llama2"
            },
            "temperature": {
                "label": "Temperature",
                "description": "The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "streaming": {
                "label": "Streaming"
            },
            "jsonMode": {
                "label": "JSON Mode",
                "description": "Coerces model outputs to only return JSON. Specify in the system prompt to return JSON. Ex: Format all responses as JSON object"
            },
            "keepAlive": {
                "label": "Keep Alive",
                "description": "How long to keep connection alive. A duration string (such as \"10m\" or \"24h\")"
            },
            "topP": {
                "label": "Top P",
                "description": "Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "topK": {
                "label": "Top K",
                "description": "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "mirostat": {
                "label": "Mirostat",
                "description": "Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "mirostatEta": {
                "label": "Mirostat ETA",
                "description": "Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "mirostatTau": {
                "label": "Mirostat TAU",
                "description": "Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "numCtx": {
                "label": "Context Window Size",
                "description": "Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "numGpu": {
                "label": "Number of GPU",
                "description": "The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "numThread": {
                "label": "Number of Thread",
                "description": "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "repeatLastN": {
                "label": "Repeat Last N",
                "description": "Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "repeatPenalty": {
                "label": "Repeat Penalty",
                "description": "Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "stop": {
                "label": "Stop Sequence",
                "placeholder": "AI assistant:",
                "description": "Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            },
            "tfsZ": {
                "label": "Tail Free Sampling",
                "description": "Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details"
            }
        }
    },
    "chatHuggingFace": {
        "label": "ChatHuggingFace",
        "description": "Wrapper around HuggingFace large language models",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "model": {
                "label": "Model",
                "placeholder": "gpt2",
                "description": "If using own inference endpoint, leave this blank"
            },
            "endpoint": {
                "label": "Endpoint",
                "placeholder": "https://xyz.eu-west-1.aws.endpoints.huggingface.cloud/gpt2",
                "description": "Using your own inference endpoint"
            },
            "temperature": {
                "label": "Temperature",
                "description": "Temperature parameter may not apply to certain model. Please check available model parameters"
            },
            "maxTokens": {
                "label": "Max Tokens",
                "description": "Max Tokens parameter may not apply to certain model. Please check available model parameters"
            },
            "topP": {
                "label": "Top Probability",
                "description": "Top Probability parameter may not apply to certain model. Please check available model parameters"
            },
            "hfTopK": {
                "label": "Top K",
                "description": "Top K parameter may not apply to certain model. Please check available model parameters"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty",
                "description": "Frequency Penalty parameter may not apply to certain model. Please check available model parameters"
            },
            "stop": {
                "label": "Stop Sequence",
                "placeholder": "AI assistant:",
                "description": "Sets the stop sequences to use. Use comma to seperate different sequences."
            }
        }
    },
    "awsChatBedrock": {
        "label": "AWS ChatBedrock",
        "description": "Wrapper around AWS Bedrock large language models that use the Converse API",
        "category": "Chat Models",
        "credential": {
            "label": "AWS Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "region": {
                "label": "Region"
            },
            "model": {
                "label": "Model Name"
            },
            "customModel": {
                "label": "Custom Model Name",
                "description": "If provided, will override model selected from Model Name option"
            },
            "streaming": {
                "label": "Streaming"
            },
            "temperature": {
                "label": "Temperature",
                "description": "Temperature parameter may not apply to certain model. Please check available model parameters"
            },
            "max_tokens_to_sample": {
                "label": "Max Tokens to Sample",
                "description": "Max Tokens parameter may not apply to certain model. Please check available model parameters"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "latencyOptimized": {
                "label": "Latency Optimized",
                "description": "Enable latency optimized configuration for supported models. Refer to the supported <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html\" target=\"_blank\">latecny optimized models</a> for more details."
            }
        }
    },
    "chatCohere": {
        "label": "ChatCohere",
        "description": "Wrapper around Cohere Chat Endpoints",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            }
        }
    },
    "chatAlibabaTongyi": {
        "label": "ChatAlibabaTongyi",
        "description": "Wrapper around Alibaba Tongyi Chat Endpoints",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            }
        }
    },
    "chatBaiduWenxin": {
        "label": "ChatBaiduWenxin",
        "description": "Wrapper around BaiduWenxin Chat Endpoints",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            }
        }
    },
    "chatCerebras": {
        "label": "ChatCerebras",
        "description": "Wrapper around Cerebras Inference API",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "basepath": {
                "label": "BasePath"
            },
            "baseOptions": {
                "label": "BaseOptions"
            }
        }
    },
    "chatCometAPI": {
        "label": "ChatCometAPI",
        "description": "Wrapper around CometAPI large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name",
                "description": "Enter the model name (e.g., gpt-5-mini, claude-sonnet-4-20250514, gemini-2.0-flash)"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "baseOptions": {
                "label": "Base Options",
                "description": "Additional options to pass to the CometAPI client. This should be a JSON object."
            }
        }
    },
    "chatFireworks": {
        "label": "ChatFireworks",
        "description": "Wrapper around Fireworks Chat Endpoints",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            }
        }
    },
    "chatGoogleVertexAI": {
        "label": "ChatGoogleVertexAI",
        "description": "Wrapper around VertexAI large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential",
            "description": "Google Vertex AI credential. If you are using a GCP service like Cloud Run, or if you have installed default credentials on your local machine, you do not need to set this credential."
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "region": {
                "label": "Region",
                "description": "Region to use for the model."
            },
            "modelName": {
                "label": "Model Name"
            },
            "customModelName": {
                "label": "Custom Model Name",
                "description": "Custom model name to use. If provided, it will override the model selected"
            },
            "temperature": {
                "label": "Temperature"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxOutputTokens": {
                "label": "Max Output Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "topK": {
                "label": "Top Next Highest Probability Tokens",
                "description": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive"
            },
            "thinkingBudget": {
                "label": "Thinking Budget",
                "description": "Number of tokens to use for thinking process (0 to disable)"
            }
        }
    },
    "chatIBMWatsonx": {
        "label": "ChatIBMWatsonx",
        "description": "Wrapper around IBM watsonx.ai foundation models",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty",
                "description": "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
            },
            "logprobs": {
                "label": "Log Probs",
                "description": "Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."
            },
            "n": {
                "label": "N",
                "description": "How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."
            },
            "presencePenalty": {
                "label": "Presence Penalty",
                "description": "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
            },
            "topP": {
                "label": "Top P",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
            }
        }
    },
    "chatLitellm": {
        "label": "ChatLitellm",
        "description": "Connect to a Litellm server using OpenAI-compatible API",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "basePath": {
                "label": "Base URL"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top P"
            },
            "timeout": {
                "label": "Timeout"
            }
        }
    },
    "chatLocalAI": {
        "label": "ChatLocalAI",
        "description": "Use local LLMs like llama.cpp, gpt4all using LocalAI",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "basePath": {
                "label": "Base Path"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "timeout": {
                "label": "Timeout"
            }
        }
    },
    "chatNemoGuardrails": {
        "label": "Chat Nemo Guardrails",
        "description": "Access models through the Nemo Guardrails API",
        "category": "Chat Models",
        "inputs": {
            "configurationId": {
                "label": "Configuration ID"
            },
            "baseUrl": {
                "label": "Base URL"
            }
        }
    },
    "chatNvdiaNIM": {
        "label": "Chat NVIDIA NIM",
        "description": "Wrapper around NVIDIA NIM Inference API",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "basePath": {
                "label": "Base Path",
                "description": "Specify the URL of the deployed NIM Inference API"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "baseOptions": {
                "label": "Base Options"
            }
        }
    },
    "chatOpenAICustom": {
        "label": "ChatOpenAI Custom",
        "description": "Custom/FineTuned model using OpenAI Chat compatible API",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "basepath": {
                "label": "BasePath"
            },
            "baseOptions": {
                "label": "BaseOptions"
            }
        }
    },
    "chatOpenRouter": {
        "label": "ChatOpenRouter",
        "description": "Wrapper around Open Router Inference API",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "basepath": {
                "label": "BasePath"
            },
            "baseOptions": {
                "label": "BaseOptions"
            }
        }
    },
    "chatPerplexity": {
        "label": "ChatPerplexity",
        "description": "Wrapper around Perplexity large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "model": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top P"
            },
            "topK": {
                "label": "Top K"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "streaming": {
                "label": "Streaming"
            },
            "timeout": {
                "label": "Timeout"
            },
            "proxyUrl": {
                "label": "Proxy Url"
            }
        }
    },
    "chatSambanova": {
        "label": "ChatSambanova",
        "description": "Wrapper around Sambanova Chat Endpoints",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "basepath": {
                "label": "BasePath"
            },
            "baseOptions": {
                "label": "BaseOptions"
            }
        }
    },
    "chatTogetherAI": {
        "label": "ChatTogetherAI",
        "description": "Wrapper around TogetherAI large language models",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name",
                "description": "Refer to <a target=\"_blank\" href=\"https://docs.together.ai/docs/inference-models\">models</a> page"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            }
        }
    },
    "chatXAI": {
        "label": "ChatXAI",
        "description": "Wrapper around Grok from XAI",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "allowImageUploads": {
                "label": "Allow Image Uploads",
                "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details."
            }
        }
    },
    "chatDeepseek": {
        "label": "ChatDeepseek",
        "description": "Wrapper around Deepseek large language models that use the Chat endpoint",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top Probability"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "stopSequence": {
                "label": "Stop Sequence",
                "description": "List of stop words to use when generating. Use comma to separate multiple stop words."
            },
            "baseOptions": {
                "label": "Base Options",
                "description": "Additional options to pass to the Deepseek client. This should be a JSON object."
            }
        }
    },
    "moduoduoPro": {
        "label": "Moduoduo Pro",
        "description": "Moduoduo Pro unified AI model gateway interface",
        "category": "Chat Models",
        "credential": {
            "label": "Connect Credential"
        },
        "inputs": {
            "cache": {
                "label": "Cache"
            },
            "modelName": {
                "label": "Model Name"
            },
            "temperature": {
                "label": "Temperature"
            },
            "streaming": {
                "label": "Streaming"
            },
            "maxTokens": {
                "label": "Max Tokens"
            },
            "topP": {
                "label": "Top P"
            },
            "frequencyPenalty": {
                "label": "Frequency Penalty"
            },
            "presencePenalty": {
                "label": "Presence Penalty"
            },
            "timeout": {
                "label": "Timeout"
            },
            "stopSequence": {
                "label": "Stop Sequence",
                "description": "List of stop words to use when generating. Use comma to separate multiple stop words."
            },
            "proxyUrl": {
                "label": "Proxy Url"
            },
            "baseOptions": {
                "label": "Base Options"
            }
        }
    }
}
